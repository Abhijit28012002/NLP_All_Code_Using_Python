{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\"A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations\"'\n",
    "\n",
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff7a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60650815",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "for i in range(0, length-seq_length, 1):\n",
    "   sequence = text[i:i + seq_length]\n",
    "   label =text[i + seq_length]\n",
    "   X.append([char_to_n[char] for char in sequence])\n",
    "   Y.append(char_to_n[label])\n",
    "\n",
    "print(\"Test Data - \",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284effc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "# Load and preprocess the data\n",
    "text = brown.words()\n",
    "\n",
    "# Create a bigram language model\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = ConditionalFreqDist(bigrams)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"the quick\"\n",
    "generated_text = seed_text\n",
    "for i in range(10):\n",
    "    # Find the next word using the bigram model\n",
    "    next_word = str(max(cfd[seed_text].values()))\n",
    "    generated_text +=\" \"+next_word\n",
    "    seed_text = next_word\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download any required datasets (e.g., stopwords, punkt tokenizer)\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf24680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import reuters  # You can replace this with your own text corpus\n",
    "\n",
    "\n",
    "\n",
    "# Load your corpus (replace 'reuters' with your own text data)\n",
    "corpus = reuters.words()\n",
    "\n",
    "# Preprocess the text data\n",
    "words = [word.lower() for word in corpus if word.isalpha()]\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "# Define a function to generate text\n",
    "def generate_text(seed_word, num_words=50):\n",
    "    text = [seed_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        next_word_candidates = [word for word in fdist if word[0] == text[-1] and len(word) > 1]\n",
    "        if next_word_candidates:\n",
    "            next_word = random.choice(next_word_candidates)\n",
    "            text.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "# Set a seed word and generate text\n",
    "seed_word = \"stock\"\n",
    "generated_text = generate_text(seed_word)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a034ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading model.safetensors: 100%|████████████████████████████████████████████████| 548M/548M [01:39<00:00, 5.49MB/s]\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)neration_config.json: 100%|████████████████████████████████████████████████████| 124/124 [00:00<?, ?B/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n",
      "The world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can choose other variations like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the seed text and generate text\n",
    "seed_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set the length of the generated text (adjust as needed)\n",
    "max_length = 100\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7df005",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5d154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n",
      "The world that was created was not the same as the one that is now. It was an endless, endless world. And the Gods were not born of nothing. They were created of a single, single thing. That was why the universe was so beautiful. Because the cosmos was made of two\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can choose other variations like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the seed text and generate text\n",
    "seed_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set the length of the generated text (adjust as needed)\n",
    "max_length = 100\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69d544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
